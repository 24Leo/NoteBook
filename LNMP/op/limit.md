首先明确一点：爬虫和翻爬虫的战争中，爬虫一定会胜利。我们不可能战胜所有的爬虫，我们需要做的是：将爬虫的成本尽可能的提高到与收益不成正比的水平，越高越好～

##方向
* 限流某个接口的总并发/请求数
    * 因为粒度比较细，可以为每个接口都设置相应的阀值。
* 限流某个接口的时间窗请求数
* 针对某些特定用户限流
    * 频繁请求
    
### 标准方案
常见的爬虫有如下规律：
    * 单一IP非常规的访问频次、不合理的流量
    * ua、refer、cookie异常；
    * 大量重复简单的网站浏览行为 （绝大多数情况，只有机器人才会做深翻页）
    * 只下载网页，没有后续的js、CSS请求 （在应用层）
    * 通过一些陷阱来发现爬虫，例如一些通过CSS对用户影藏的链接，只有爬虫才会访问；

####常见分析策略
* ua、refer
    * 爬虫一般会设置自己的ua为：spider。但是也有不自觉的伪装成某一种浏览器，但是好在它的值不会经常变动。
    * 一般情况下，爬虫等不合理的流量其ua或者refer都是空，所以最简单判定就是：如果ua、refer为空，那么限制策略可以定位：同一个IP，1分钟2次。
* IP
    * 如果一个IP请求的只有HTML，没有js、CSS的话，基本可以断定是爬虫
    * 这个一般比较多，比如现在文章资讯详情页不断不被扫描，这些IP就只会访问详情页，而不请求CSS、JS等文件。所以根据这个情况就可以限制很多IP。但是有一个问题就是IP太多了，而且更换IP的代价有比较低，所以实现起来没有太大意义；
* cookie
    * 有些公司/网站会对cookie处理，添加ID等。

[return](README.md)