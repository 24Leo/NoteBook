首先明确一点：爬虫和翻爬虫的战争中，爬虫一定会胜利。我们不可能战胜所有的爬虫，我们需要做的是：将爬虫的成本尽可能的提高到与收益不成正比的水平，越高越好～

##方向
* 限流某个接口的总并发/请求数
    * 因为粒度比较细，可以为每个接口都设置相应的阀值。
* 限流某个接口的时间窗请求数
* 针对某些特定用户限流
    * 频繁请求
    
##限流算法
###计数器
计数器算法是最简单、最暴力、最容易实现的算法。它的原理是：对应A接口，我们设定1分钟内超过100次就限流。那么每当一次请求过来就对计数器加1，如果计数器值超过100并且和第一次请求还在1分钟内，那么就会触发限流：
图片
####代码实现
```java
public class CounterDemo {
	public long timeStamp = getNowTime();
	public int reqCount = 0;
	public final int limit = 100;          // 时间窗口内最大请求数
	public final long interval = 1000;     // 时间窗口ms
	public boolean grant() {
		long now = getNowTime();
		if (now < timeStamp + interval) {
			// 在时间窗口内
			reqCount++;
			// 判断当前时间窗口内是否超过最大请求控制数
			return reqCount <= limit;
		}
		else {
	       // 每隔1分钟重置起始时间和计数器
			timeStamp = now;
			// 超时后重置
			reqCount = 1;
			return true;
		}
	}
}
```
####分析
可以看到，如果当前时间在起始时间加窗口间隔范围内，计数器加1，返回是否超过指定上限。如果超过时间间隔，那么起始时间重置，计数器重置。
由上述原理和代码分析可以，计数器存在一个漏洞：
图片
有一个黑产在一分钟的前59s内无请求，在第59s内时发送100个请求，限流逻辑判断这一分钟刚好达到100个请求不会触发限流（我们假定此时无普通用户，如果有的话也无所谓：我们在模拟极限情况。重点是后一分钟的第一秒）。然后1s后限流逻辑会重置，这个黑产第60s内（第二个分钟的第一秒）再次发生100个请求，限流逻辑发现这一分钟刚好也只有100个请求，不会限流（但是这一分钟的后59s都会被限流）。
* 我们发现极限情况下59s和60s这1s中内服务器接收到了200个请求，原本我们以为1分钟100个请求，QPS应该是1.7左右，但是极限情况下可以打到200QPS，从而瞬间压垮我们的服务。
* 我们发现其实主要是我们统计精度不够精细的原因，但是即使我们把精度设置为1S也是有问题，因为总有更细的粒度（1s也是第一个毫秒和最后一个毫秒）

###滑动窗口
为了解决计数器算法的bug，前人引入TCP协议的滑动窗口的概念：
图片
从上图中我们将1分钟分成6个格子：每个格子代表10s，同时每个格子有自己独立的计数器。
    * 比如当一个请求 在0:35秒的时候到达，那么0:30~0:39对应的counter就会加1。
那么是怎么解决计数器算法的bug呢？
    * 我们可以看到59s发送100个请求，最后一个小窗口计数器为100，整个大窗口此时也是100个。那么滑动到下一秒中（最后一个小窗口为倒数第二个，新1s为最后一个小窗口），如果发现又来了100个请求，那么最后一个小窗口计数器逐步增加100，但是加到1的时候就发现整个大窗口此时已经达到了101触发了限流。
    * 其实我们分析一下计数器算法其实也是滑动窗口，只是其只有一个窗口而已
        * **计数器算法就是单窗口滑动算法**
        
###漏斗

###令牌桶

### 标准方案
常见的爬虫有如下规律：
    * 单一IP非常规的访问频次、不合理的流量
    * ua、refer、cookie异常；
    * 大量重复简单的网站浏览行为 （绝大多数情况，只有机器人才会做深翻页）
    * 只下载网页，没有后续的js、CSS请求 （在应用层）
    * 通过一些陷阱来发现爬虫，例如一些通过CSS对用户影藏的链接，只有爬虫才会访问；

####常见分析策略
* ua、refer
    * 爬虫一般会设置自己的ua为：spider。但是也有不自觉的伪装成某一种浏览器，但是好在它的值不会经常变动。
    * 一般情况下，爬虫等不合理的流量其ua或者refer都是空，所以最简单判定就是：如果ua、refer为空，那么限制策略可以定位：同一个IP，1分钟2次。
* IP
    * 如果一个IP请求的只有HTML，没有js、CSS的话，基本可以断定是爬虫
    * 这个一般比较多，比如现在文章资讯详情页不断不被扫描，这些IP就只会访问详情页，而不请求CSS、JS等文件。所以根据这个情况就可以限制很多IP。但是有一个问题就是IP太多了，而且更换IP的代价有比较低，所以实现起来没有太大意义；
* cookie
    * 有些公司/网站会对cookie处理，添加ID等。
    
####限制动作
通过上面分析，我们需要梳理出核心接口以及普通接口，然后针对不同的接口制定特定的策略。但是这只是第一步，如何针对不同的策略制定相应的应对策略，也就是后面对应的“动作”也同样重要。假如我们命中了设置的某一个策略，那么哪些接口不需要判断直接close，那些接口需要判断是否是正常用户访问（比如让用户输入验证码）？另外正常用户判定后，如何保证用户在一段时间内不需要继续判定。
* 怎么判定？
    * 可以对命中策略的请求在其header中添加一个字段，然后我们在业务层判断是否有该header以及验证码有效时间来跳转到验证码页面，输入正确后在跳转回正常页面；
    * add_header
* 用户判定后生效时间段确定？
    * 第一次验证码判定正确后，我们不可能接下来对每一次访问都进行验证码处理。所以第一次验证码判定后，生效时间段的制定、两次验证码之间间隔设定都需要后端处理实现。
    * 图片
				
[return](README.md)