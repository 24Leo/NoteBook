首先明确一点：爬虫和翻爬虫的战争中，爬虫一定会胜利。我们不可能战胜所有的爬虫，我们需要做的是：将爬虫的成本尽可能的提高到与收益不成正比的水平，越高越好～

##方向
* 限流某个接口的总并发/请求数
    * 因为粒度比较细，可以为每个接口都设置相应的阀值。
* 限流某个接口的时间窗请求数
* 针对某些特定用户限流
    * 频繁请求
    
##限流算法
###计数器
计数器算法是最简单、最暴力、最容易实现的算法。它的原理是：对应A接口，我们设定1分钟内超过100次就限流。那么每当一次请求过来就对计数器加1，如果计数器值超过100并且和第一次请求还在1分钟内，那么就会触发限流：
![](/assets/2016-09-01_20_31_28.jpg)。
####代码实现
```PHP
public class CounterDemo {
	public long timeStamp = getNowTime();
	public int reqCount = 0;
	public final int limit = 100;          // 时间窗口内最大请求数
	public final long interval = 1000;     // 时间窗口ms
	public boolean grant() {
		long now = getNowTime();
		if (now < timeStamp + interval) {
			// 在时间窗口内
			reqCount++;
			// 判断当前时间窗口内是否超过最大请求控制数
			return reqCount <= limit;
		}
		else {
	                // 每隔1分钟重置时间和计数器
			timeStamp = now;
			// 超时后重置
			reqCount = 1;
			return true;
		}
	}
}
```

###滑动窗口

**其实计数器算法就是单滑动窗口算法**
###漏斗
###令牌桶

### 标准方案
常见的爬虫有如下规律：
    * 单一IP非常规的访问频次、不合理的流量
    * ua、refer、cookie异常；
    * 大量重复简单的网站浏览行为 （绝大多数情况，只有机器人才会做深翻页）
    * 只下载网页，没有后续的js、CSS请求 （在应用层）
    * 通过一些陷阱来发现爬虫，例如一些通过CSS对用户影藏的链接，只有爬虫才会访问；

####常见分析策略
* ua、refer
    * 爬虫一般会设置自己的ua为：spider。但是也有不自觉的伪装成某一种浏览器，但是好在它的值不会经常变动。
    * 一般情况下，爬虫等不合理的流量其ua或者refer都是空，所以最简单判定就是：如果ua、refer为空，那么限制策略可以定位：同一个IP，1分钟2次。
* IP
    * 如果一个IP请求的只有HTML，没有js、CSS的话，基本可以断定是爬虫
    * 这个一般比较多，比如现在文章资讯详情页不断不被扫描，这些IP就只会访问详情页，而不请求CSS、JS等文件。所以根据这个情况就可以限制很多IP。但是有一个问题就是IP太多了，而且更换IP的代价有比较低，所以实现起来没有太大意义；
* cookie
    * 有些公司/网站会对cookie处理，添加ID等。
    
####限制动作
通过上面分析，我们需要梳理出核心接口以及普通接口，然后针对不同的接口制定特定的策略。但是这只是第一步，如何针对不同的策略制定相应的应对策略，也就是后面对应的“动作”也同样重要。假如我们命中了设置的某一个策略，那么哪些接口不需要判断直接close，那些接口需要判断是否是正常用户访问（比如让用户输入验证码）？另外正常用户判定后，如何保证用户在一段时间内不需要继续判定。
* 怎么判定？
    * 可以对命中策略的请求在其header中添加一个字段，然后我们在业务层判断是否有该header以及验证码有效时间来跳转到验证码页面，输入正确后在跳转回正常页面；
    * add_header
* 用户判定后生效时间段确定？
    * 第一次验证码判定正确后，我们不可能接下来对每一次访问都进行验证码处理。所以第一次验证码判定后，生效时间段的制定、两次验证码之间间隔设定都需要后端处理实现。
    * ![](/assets/linit.png)

[return](README.md)